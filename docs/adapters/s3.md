```markdown
# Amazon S3

- Install extra: `.[s3]` (package: `s3fs`)
- Common env vars / config: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN` (optional), or use IAM role when running on AWS.
- URI scheme: `s3://bucket/path` used by fsspec-backed datasources.
- Caveats: `s3fs` relies on `botocore`; watch for large object handling, multipart, and eventual consistency for listings.

Example (env vars + pandas):

```python
import pandas as pd

# set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY (or rely on IAM role)
df = pd.read_parquet('s3://my-bucket/sample_data/customers.parquet')
```

Example `profiles.yml` snippet for a GE pandas_filesystem datasource using S3:

```yaml
datasources:
  my_s3:
    class_name: Datasource
    execution_engine:
      class_name: PandasExecutionEngine
    reader_options:
      storage_options:
        profile_name: default
```

```
# Amazon S3

- Install extra: `.[s3]` (package: `s3fs`)
- Common env vars / config: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_SESSION_TOKEN` (optional), or use IAM role when running on AWS.
- URI scheme: `s3://bucket/path` used by fsspec-backed datasources.
- Caveats: `s3fs` relies on `botocore`; watch for large object handling, multipart, and eventual consistency for listings.
